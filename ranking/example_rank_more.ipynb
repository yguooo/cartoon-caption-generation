{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5602ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjiayi/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/chenjiayi/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from rank_more import prepare, Evaluation\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a5a29",
   "metadata": {},
   "source": [
    "Since all of our evaluators use OpenAI API, the first step is to set up your OpenAI API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785dcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Your OpenAI API Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4feae",
   "metadata": {},
   "source": [
    "## Overall Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f1413",
   "metadata": {},
   "source": [
    "### Generated Caption Model: Mistral-7B SFT, Evaluator: GPT4turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efd5e7",
   "metadata": {},
   "source": [
    "We evaluate various models that generate captions by comparing the generated captions against four groups of human contestant entries at different ranking levels, which include top10, #200-#209, #1000-#1009, and contestant median.\n",
    "\n",
    "As concluded based on Table 2 in paper, we use GPT4-Turbo as evaluator with descriptions from Hessel et al. in Overall Comparison and GPT4o-vision as evaluator with raw cartoon images in Best Pick Comparison.\n",
    "\n",
    "In overall comparison, the evaluator compares the overall funniness of the group of model-generated captions against each group of contestant-submitted captions. In best pick comparison, the evaluator first pick the funniest caption from each of the two groups and then choose the funnier caption accordingly.\n",
    "\n",
    "Our total number of cartoons to be evaluated is 91. In this jupyter notebook, we run overall comparison over 91 cartoons completely to show win rates and run best pick comparison on one cartoon to present a example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a781bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:43<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate against top10 captions: 3.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [03:10<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate against #200-#209 captions: 3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:45<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate against #1000-#1009 captions: 8.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:37<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate against contestant median captions: 14.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "descriptions, human_captions_top, human_captions_200, human_captions_1k, human_captions_median = prepare(Overall = True)\n",
    "file_path = \"/Users/chenjiayi/Desktop/humor/gy_model/caption_model/sft_gpto4_iter200_results_parsed.csv\"\n",
    "\n",
    "evalTop = Evaluation(Overall = True, Top = True, humanTop = human_captions_top, \n",
    "                    file_path = file_path, descriptions = descriptions, apiKey = api_key)\n",
    "accTop = evalTop.eval()\n",
    "print(\"Win rate against top10 captions:\", accTop)\n",
    "\n",
    "eval200 = Evaluation(Overall = True, T200 = True, human200 = human_captions_200, \n",
    "                    file_path = file_path, descriptions = descriptions, apiKey = api_key)\n",
    "acc200 = eval200.eval()\n",
    "print(\"Win rate against #200-#209 captions:\", acc200)\n",
    "\n",
    "eval1k = Evaluation(Overall = True, T1k = True, human1k = human_captions_1k, \n",
    "                    file_path = file_path, descriptions = descriptions, apiKey = api_key)\n",
    "acc1k = eval1k.eval()\n",
    "print(\"Win rate against #1000-#1009 captions:\", acc1k)\n",
    "\n",
    "evalMed = Evaluation(Overall = True, Median = True, humanMedian = human_captions_median, \n",
    "                    file_path = file_path, descriptions = descriptions, apiKey = api_key)\n",
    "accMed = evalMed.eval()\n",
    "print(\"Win rate against contestant median captions:\", accMed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce53e33",
   "metadata": {},
   "source": [
    "## Best Pick Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba023283",
   "metadata": {},
   "source": [
    "### Generated Caption Model: Claude, Evaluator: GPT4oV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc1def1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.25s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.32s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.90s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n"
     ]
    }
   ],
   "source": [
    "img, human_captions_top, human_captions_200, human_captions_1k, human_captions_median = prepare(BestPick = True)\n",
    "file_path = \"/Users/chenjiayi/Desktop/humor/D/D_captions/claude_cap.csv\"\n",
    "\n",
    "evalTop = Evaluation(BestPick = True, Top = True, humanTop = human_captions_top, AI = True,\n",
    "                    file_path = file_path, img = img, apiKey = api_key, num_pairs = 1)\n",
    "accTop = evalTop.eval()\n",
    "\n",
    "eval200 = Evaluation(BestPick = True, T200 = True, human200 = human_captions_200, AI = True,\n",
    "                    file_path = file_path, img = img, apiKey = api_key, num_pairs = 1)\n",
    "acc200 = eval200.eval()\n",
    "\n",
    "eval1k = Evaluation(BestPick = True, T1k = True, human1k = human_captions_1k, AI = True,\n",
    "                    file_path = file_path, img = img, apiKey = api_key, num_pairs = 1)\n",
    "acc1k = eval1k.eval()\n",
    "\n",
    "evalMed = Evaluation(BestPick = True, Median = True, humanMedian = human_captions_median, AI = True,\n",
    "                    file_path = file_path, img = img, apiKey = api_key, num_pairs = 1)\n",
    "accMed = evalMed.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
