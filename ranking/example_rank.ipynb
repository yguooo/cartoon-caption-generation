{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6887cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjiayi/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/chenjiayi/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from rank import prepare_DescriptionModel, prepare_VisionModel, Ranking\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2108e3cd",
   "metadata": {},
   "source": [
    "Since all of our evaluators use OpenAI API, the first step is to set up your OpenAI API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de415466",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Your OpenAI API Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6457fc",
   "metadata": {},
   "source": [
    "## Pairwise Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e8add",
   "metadata": {},
   "source": [
    "### Evaluator: gpt4turbo, Description: Hessel's "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6be78eb2",
   "metadata": {},
   "source": [
    "To obtain the accuracy of description models that use descriptions of cartoons when evaluating captions, we need to first call the function named 'prepare_DescriptionModel'.\n",
    "\n",
    "There are three string values for 'comparison_method' parameter in all our functions or classes: \"Pairwise\", \"Overall\", and \"BestPick\". \n",
    "\n",
    "\"Pairwise\" is comparing two candidate captions at a time, while \"Overall\" and \"BestPick\" compare groups of ten captions from different sources, such as human submissions from different ranking levels, or captions generated by different language models. \n",
    "\n",
    "In overall comparisons, the evaluator compares the overall funniness of the group of model-generated captions against each group of contestant-submitted captions. In best pick comparisons, the evaluator first pick the funniest caption from each of the two groups and then choose the funnier caption accordingly.\n",
    "\n",
    "When we want to compare two candidate captions at a time with descriptions from Hessel et al., we need to set 'comparison_method=\"Pairwise\", Hessel=True' so that the 'prepare_DescriptionModel' function can return a dataframe of Hessel's dataset. This is all we need for pairwise comparison with descriptions from Hessel et al. Then, we can create an instance of the 'Ranking' class.\n",
    "\n",
    "In 'Ranking', 'annotation_type' can be either \"Description\" or \"Image\", and 'description_generator' can be \"gpt-4-vision\", \"gpt-4-turbo\", or \"Hessel\". When initializing Ranking, the necessary arguments are 'comparison_method', 'evaluator', 'annotation_type', 'description_generator', 'apiKey', and other arguments specific to the comparisons. After the initialization, we can get the ranking accuracy by calling the 'rank()' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56eab11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testing = prepare_DescriptionModel(comparison_method = \"Pairwise\", Hessel = True)\n",
    "model = Ranking(comparison_method = \"Pairwise\", evaluator = \"gpt-4-turbo\", annotation_type = \"Description\",\n",
    "                  description_generator = \"Hessel\", testing = testing, apiKey = api_key, num_pairs = 1)\n",
    "accuracy = model.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7420d16",
   "metadata": {},
   "source": [
    "### Evaluator: gpt4oV, Raw image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9536581c",
   "metadata": {},
   "source": [
    "To obtain the accuracy of vision models that use raw images of cartoons when evaluating captions, we need to first call the function named 'prepare_VisionModel'. \n",
    "\n",
    "The 'image_pairs' returned by the 'prepare_VisionModel' function is the only argument specific to the comparisons using the vision model. It is a list of '[contest_number, base64_image, captionA, captionB, label]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6b265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_pairs = prepare_VisionModel(comparison_method = \"Pairwise\")\n",
    "model = Ranking(comparison_method = \"Pairwise\", evaluator = \"gpt-4o\", annotation_type = \"Image\",\n",
    "                 image_pairs = image_pairs, apiKey = api_key, num_pairs = 1)\n",
    "accuracy = model.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedabbc",
   "metadata": {},
   "source": [
    "## Overall Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a57ea",
   "metadata": {},
   "source": [
    "### Evaluator: gpt4turbo, Description Generator: gpt4V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d97c8ae",
   "metadata": {},
   "source": [
    "To use descriptions generated by specific models, such as gpt4V, we need to obtain a dataframe of descriptions for 100 testing cartoons, which correspond to 500 example cartoons based on our 5-shot technique. \n",
    "\n",
    "The dataframe is called 'Dtesting' and is composed of three columns: ['cnum', 'canny', 'uncanny'], representing ['contest_number', 'canny description', 'uncanny description']. 'Deg' is a dataframe containing descriptions of all distinct cartoons in Hessel's dataset generated by specific models, such as gpt4V. We select 500 examples from it by setting a random seed.\n",
    "\n",
    "With 'Dtesting' and 'Deg' prepared, we can call 'prepare_DescriptionModel'. \n",
    "\n",
    "Unlike pairwise comparison using descriptions from Hessel et al., overall comparison and best pick comparison also need the input of 'Deg' and 'Dtesting' besides setting 'comparison_method=\"Overall\", GPT4V=True' or 'comparison_method=\"BestPick\", GPT4V=True'. \n",
    "\n",
    "The 'prepare_DescriptionModel' function will then return four variables: 'deg', 'eg', 'dtesting', and 'captions'. 'deg' is a list of '[canny, uncanny]' descriptions for 600 example cartoons, and 'eg' is a list of 600 captions and labels (we actually use only 500 example cartoons' information; please see the code for details). 'dtesting' is a list of descriptions for 100 testing cartoons grabbed from 'Dtesting'. 'captions' is a list of human contestant entries at the top 10 and #1000-#1009.\n",
    "\n",
    "What's more, in group comparisons, for each cartoon, we flip the groups of captions to apply the recalibrated decision rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01541ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/chenjiayi/Desktop/humor/D/gpt4V_100descriptions'\n",
    "\n",
    "Dtesting = pd.DataFrame(columns=['cnum', 'canny', 'uncanny']) # description df for 100 testing cartoons\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename != \".DS_Store\":\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        Dtesting = pd.concat([Dtesting, df], ignore_index=True)\n",
    "\n",
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4V_description/example.csv'\n",
    "Deg1 = pd.read_csv(file_path)\n",
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4V_description/example_new.csv'\n",
    "Deg2 = pd.read_csv(file_path)\n",
    "Deg = pd.concat([Deg1, Deg2], ignore_index=True)\n",
    "\n",
    "deg, eg, dtesting, captions = prepare_DescriptionModel(comparison_method = \"Overall\", GPT4V = True, \n",
    "                                                       Deg = Deg, Dtesting = Dtesting)\n",
    "\n",
    "model = Ranking(comparison_method = \"Overall\", evaluator = \"gpt-4-turbo\", annotation_type = \"Description\",\n",
    "                description_generator = \"gpt-4-vision\", deg = deg, dtesting = dtesting, eg = eg,\n",
    "                captions = captions, apiKey = api_key, num_pairs = 1)\n",
    "accuracy = model.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f134213",
   "metadata": {},
   "source": [
    "### Evaluator: gpt4turbo-V, Raw image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f73cdb1c",
   "metadata": {},
   "source": [
    "To perform group comparisons, such as \"Overall\" and \"BestPick\", we fix the list of contest numbers to be evaluated to better observe the performance of different evaluators. The list used in the description model is named 'cartoons', and the one used in the vision model is named 'cartoons_v'. These lists should be set as arguments when calling prepare_VisionModel.\n",
    "\n",
    "When doing group comparisons, 'prepare_VisionModel' will also return 'img' and 'captions' besides 'image_pairs', which are the images and human contestant entries at top 10 and #1000-#1009 corresponding to the list of contest numbers to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215b05d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/chenjiayi/Desktop/humor/cartoon_forGroupComparison.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    cartoons_v = pickle.load(file)\n",
    "    \n",
    "image_pairs, img, captions = prepare_VisionModel(comparison_method = \"Overall\", \n",
    "                                                 cartoons_GroupComparison = cartoons_v)\n",
    "\n",
    "model = Ranking(comparison_method = \"Overall\", evaluator = \"gpt-4-turbo\", annotation_type = \"Image\",\n",
    "                 image_pairs = image_pairs, img = img, captions = captions, apiKey = api_key, num_pairs = 1)\n",
    "accuracy = model.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d051e12",
   "metadata": {},
   "source": [
    "## Best Pick Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396a1dc",
   "metadata": {},
   "source": [
    "### Evaluator: gpt4turbo, Description Generator: gpt4oV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2f8986",
   "metadata": {},
   "source": [
    "Similarly, with gpt4oV-generated descriptions of 100 testing cartoons and all distinct cartoons in Hessel's dataset prepared, we can call 'prepare_DescriptionModel' first and 'Ranking' then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9721822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4oV_description/cartoon_pairwise_o.csv'\n",
    "Dtesting = pd.read_csv(file_path)\n",
    "Dtesting = Dtesting[['cnum', 'canny', 'uncanny']] # description df for 100 testing cartoons\n",
    "\n",
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4oV_description/example_o.csv'\n",
    "Deg1 = pd.read_csv(file_path)\n",
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4oV_description/new_example_o.csv'\n",
    "Deg2 = pd.read_csv(file_path)\n",
    "Deg = pd.concat([Deg1, Deg2], ignore_index=True)\n",
    "    \n",
    "deg, eg, dtesting, captions = prepare_DescriptionModel(comparison_method = \"BestPick\", GPT4oV = True, \n",
    "                                                       Deg = Deg, Dtesting = Dtesting)\n",
    " \n",
    "model = Ranking(comparison_method = \"BestPick\", evaluator = \"gpt-4-turbo\", annotation_type = \"Description\",\n",
    "                description_generator = \"gpt-4o-vision\", deg = deg, dtesting = dtesting, eg = eg,\n",
    "                captions = captions, apiKey = api_key, num_pairs = 1)\n",
    "accuracy = model.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71036d9",
   "metadata": {},
   "source": [
    "### Evaluator: gpt4oV, Raw image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b154c53",
   "metadata": {},
   "source": [
    "Similarly, with 'image_pairs' of example cartoons, images to be evaluated and corresponding human contestant entries at top 10 and #1000-#1009 prepared, we can call 'prepare_VisionModel' first and 'Ranking' then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ac18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_pairs, img, captions = prepare_VisionModel(comparison_method = \"Overall\", \n",
    "                                                 cartoons_GroupComparison = cartoons_v)\n",
    "\n",
    "model = Ranking(comparison_method = \"BestPick\", evaluator = \"gpt-4o\", annotation_type = \"Image\",\n",
    "                 image_pairs = image_pairs, img = img, captions = captions, apiKey = api_key, num_pairs = 1)\n",
    "accuracy = model.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7175e97",
   "metadata": {},
   "source": [
    "## Overall Comparison with evaluator gpt4turbo and gpt4V-generated description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ba46867",
   "metadata": {},
   "source": [
    "Here is how we run a complete overall comparison over 100 cartoons. The results include flip-label experiments over the 100 cartoons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da11da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:39<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of results is 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = '/Users/chenjiayi/Desktop/humor/D/gpt4V_100descriptions'\n",
    "\n",
    "Dtesting = pd.DataFrame(columns=['cnum', 'canny', 'uncanny']) # description df for 100 testing cartoons\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename != \".DS_Store\":\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        Dtesting = pd.concat([Dtesting, df], ignore_index=True)\n",
    "\n",
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4V_description/example.csv'\n",
    "Deg1 = pd.read_csv(file_path)\n",
    "file_path = '/Users/chenjiayi/Desktop/humor/D/gpt4V_description/example_new.csv'\n",
    "Deg2 = pd.read_csv(file_path)\n",
    "Deg = pd.concat([Deg1, Deg2], ignore_index=True)\n",
    "\n",
    "deg, eg, dtesting, captions = prepare_DescriptionModel(comparison_method = \"Overall\", GPT4V = True, \n",
    "                                                       Deg = Deg, Dtesting = Dtesting)\n",
    "\n",
    "model = Ranking(comparison_method = \"Overall\", evaluator = \"gpt-4-turbo\", annotation_type = \"Description\",\n",
    "                description_generator = \"gpt-4-vision\", deg = deg, dtesting = dtesting, eg = eg,\n",
    "                captions = captions, apiKey = api_key)\n",
    "accuracy = model.rank()\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
