from datasets import load_from_disk
import pandas as pd
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, AutoConfig
from peft import PeftModel
import argparse
from util import seed_everything
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from PIL import Image


def get_gen_config(model_name):
    '''
    Our default generation configuration that performs top-p sampling.
    '''
    generation_config = GenerationConfig.from_pretrained(model_name)
    generation_config.max_new_tokens = 256 # maximum number of new tokens that can be generated by the model
    generation_config.temperature = 0.7 # randomness of the generated tex
    generation_config.top_p = 0.95 # diversity of the generated text
    generation_config.do_sample = True # sampling during the generation process
    generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text
    return generation_config

def get_unique_dataset(df):
    '''
    Obtain the unique prompts from the dataset
    '''
    contest_numbers = []
    examples = []
    for example in df:
        if example['contest_number'] not in contest_numbers: 
            contest_numbers.append(example['contest_number'])
            examples.append({'contest_number': example['contest_number'], 'prompt': example['prompt']}) 
    return examples

def get_unique_dataset_llava(df):
    '''
    Obtain the unique prompts from the dataset
    '''
    contest_numbers = []
    examples = []
    for example in df:
        if example['contest_number'] not in contest_numbers: 
            contest_numbers.append(example['contest_number'])
            examples.append({'contest_number': example['contest_number'], 'image': example['image'] , 'prompt': example['prompt']}) 
    return examples

def process_generation(cell):
    '''
    Only keep the caption and discard the explanation. For zero-shot model, explanation shows up even when not explicitly requested.
    '''
    # The generation text can be a non-string when the generation model fails.
    cell = str(cell) 
    cell = cell.strip()
    # Remove quotes
    while cell.startswith('"') and cell.endswith('"'):
        cell = (cell[1:-1]).strip()
    # Only keep the first line
    cell = cell.split('\n', 1)[0]
    # Remove quotes
    while cell.startswith('"') and cell.endswith('"'):
        cell = (cell[1:-1]).strip()
    return cell

def generate_captions(model, tokenizer, test_dataset_unique, generation_config, num_generation): 
    '''
    Generate sample captions for each unique contests given the model
    '''
    prompt_df = pd.DataFrame(test_dataset_unique)
    prompt_df = prompt_df.sort_values(by=['contest_number'], ascending=[True])
    for i in range(num_generation): 
        prompt_df["caption"+str(i+1)] = ""

    for i in range(len(prompt_df)): 
        row = prompt_df.iloc[i,:]
        
        texts = [row['prompt']]*num_generation
        encoding = tokenizer(texts, padding=True, return_tensors='pt').to("cuda")
        with torch.no_grad():
            generated_ids = model.generate(**encoding, generation_config=generation_config)
        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        generated_texts = [gen[len(row['prompt'])+1 :] for gen in generated_texts]
        for j in range(num_generation): 
            # Only keep the caption and discard the explanation.
            prompt_df.iloc[i, j+2] = process_generation(generated_texts[j])
    return prompt_df

################ Save ZS Result ################

def save_zs_results(model_name, dataset_dir, output_dir, num_generation = 10, setting = ""):
    '''
    Save the caption generation result for the zero-shot model.
    '''
    test_dataset = load_from_disk(os.path.join(dataset_dir, 'zs_dataset', 'test_zs_dataset'))

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16,
        trust_remote_code=True,
        device_map="cuda",
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left" 
    generation_config = get_gen_config(model_name)

    test_dataset_unique = get_unique_dataset(test_dataset)
    
    prompt_df = generate_captions(model, tokenizer, test_dataset_unique, generation_config, num_generation)
    prompt_df.to_csv(os.path.join(output_dir, 'zs{}_gen{}.csv'.format(setting, num_generation)), index=False)

################ Save SFT Result ################

def save_sft_results(model_name, dataset_dir, output_dir, model_checkpoint, \
        new_padding_token, num_generation = 10, setting = ""):
    '''
    Save the caption generation result for the supervised finetuning model.
    '''
 
    test_dataset = load_from_disk(os.path.join(dataset_dir, 'sft_dataset', 'test_sft_dataset')) 

    if new_padding_token:   
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map="cuda",
        )
        model.resize_token_embeddings(len(tokenizer))
        peft_model = PeftModel.from_pretrained(
            model=model,
            model_id=model_checkpoint,
            device_map="cuda"
        )
        model = peft_model
    else: 
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map="cuda",
        )
        peft_model = PeftModel.from_pretrained(
            model=model,
            model_id=model_checkpoint,
            device_map="cuda"
        )
        model = peft_model

    generation_config = get_gen_config(model_name)
    test_dataset_unique = get_unique_dataset(test_dataset)

    prompt_df = generate_captions(model, tokenizer, test_dataset_unique, generation_config, num_generation)
    prompt_df.to_csv(os.path.join(output_dir, 'sft{}_gen{}.csv'.format(setting, num_generation)), index=False)

# ################ Save DPO Result ################

def save_dpo_results(model_name, dataset_dir, output_dir, model_checkpoint, num_generation = 10, setting = ""):
    '''
    Save the caption generation result for the dpo model.
    '''
    test_dataset = load_from_disk(os.path.join(dataset_dir, 'dpo_dataset', 'test_dpo_dataset')) 
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    # The DPO model requires a new padding token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16,
        trust_remote_code=True,
        device_map="cuda",
    )
    model.resize_token_embeddings(len(tokenizer))
    peft_model = PeftModel.from_pretrained(
        model=model,
        model_id=model_checkpoint,
        device_map="cuda"
    )
    model = peft_model

    generation_config = get_gen_config(model_name)
    test_dataset_unique = get_unique_dataset(test_dataset)

    prompt_df = generate_captions(model, tokenizer, test_dataset_unique, generation_config, num_generation)
    prompt_df.to_csv(os.path.join(output_dir, 'dpo{}_gen{}.csv'.format(setting, num_generation)), index=False)

# ################ Save PPO Result ################

def save_ppo_results(model_name, dataset_dir, output_dir, model_checkpoint, num_generation = 10, setting = ""):
    '''
    Save the caption generation result for the ppo model.
    '''
    test_dataset = load_from_disk(os.path.join(dataset_dir, 'dpo_dataset', 'test_dpo_dataset')) 
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_checkpoint, torch_dtype=torch.float16,
        trust_remote_code=True,
        device_map="cuda",
    )

    generation_config = get_gen_config(model_name)
    test_dataset_unique = get_unique_dataset(test_dataset)

    prompt_df = generate_captions(model, tokenizer, test_dataset_unique, generation_config, num_generation)
    prompt_df.to_csv(os.path.join(output_dir, 'ppo{}_gen{}.csv'.format(setting, num_generation)), index=False)

# ################ Save llava Result ################

def save_llava_results(model_name, dataset_dir, output_dir, model_checkpoint, num_generation = 10, setting = "", device = 'cuda:0'):
    '''
    Save the caption generation result for the llava (sft) model.
    '''
    test_dataset = load_from_disk(os.path.join(dataset_dir, 'llava_dataset', 'test_llava_dataset')) 
    test_dataset_unique = get_unique_dataset_llava(test_dataset)
    generation_config = get_gen_config(model_name)
    
    if model_checkpoint is None:
        model_checkpoint = model_name

    processor = LlavaNextProcessor.from_pretrained(model_checkpoint)
    model = LlavaNextForConditionalGeneration.from_pretrained(model_checkpoint, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)

    prompt_df = pd.DataFrame(test_dataset_unique)
    prompt_df = prompt_df.sort_values(by=['contest_number'], ascending=[True])
    for i in range(num_generation): 
        prompt_df["caption"+str(i+1)] = ""

    for i in range(len(prompt_df)): 
        row = prompt_df.iloc[i,:]
        image = Image.open(os.path.join(dataset_dir, "cartoons", row['image']))
        prompt = row['prompt']
        for j in range(num_generation): 
            inputs = processor(prompt, image, return_tensors="pt").to(device)
            # autoregressively complete prompt
            output = model.generate(**inputs, max_new_tokens=256, generation_config=generation_config)
            output = processor.decode(output[0], skip_special_tokens=True)
            output = output[len(prompt):]
            prompt_df.iloc[i, j+3] = output # Only keep the caption and discard the explanation.
    if model_checkpoint is None: 
        prompt_df.to_csv(os.path.join(output_dir, 'llava_zs{}_gen{}.csv'.format(setting, num_generation)), index=False)
    else: 
        prompt_df.to_csv(os.path.join(output_dir, 'llava_sft{}_gen{}.csv'.format(setting, num_generation)), index=False)

def main(args):
    seed_everything(2024) 
    
    args.output_dir = os.path.join(args.output_dir, 'generation')
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    if args.method == "zs":
        save_zs_results(args.model_name, args.dataset_dir, args.output_dir, num_generation=args.num_generation, setting=args.setting)
    elif args.method == "sft": 
        save_sft_results(args.model_name, args.dataset_dir, args.output_dir, args.model_checkpoint, \
            args.new_padding_token, num_generation=args.num_generation, setting=args.setting)
    elif args.method == "dpo": 
        assert args.new_padding_token , "Current implementation requires the DPO checkpoint to have a new padding token"
        save_dpo_results(args.model_name, args.dataset_dir, args.output_dir, args.model_checkpoint, \
            num_generation=args.num_generation, setting=args.setting)
    elif args.method == "ppo": 
        assert (not args.new_padding_token), "Current implementation requires the PPO checkpoint to not have a new padding token"
        save_ppo_results(args.model_name, args.dataset_dir, args.output_dir, args.model_checkpoint, \
            num_generation=args.num_generation, setting=args.setting)
    elif args.method == 'llava': 
        save_llava_results(args.model_name, args.dataset_dir, args.output_dir, args.model_checkpoint, \
            num_generation=args.num_generation, setting=args.setting, device=args.device)
    else: 
        raise ValueError("Pick a valid method from [zs, sft, dpo, ppo, llava] ")
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Description of your script")
    parser.add_argument("--dataset_dir", type=str, required=True, help="Your dataset path")
    parser.add_argument("--output_dir", type=str, required=True, help="Your output path")
    parser.add_argument("--method", type=str, required=True, help="Description for the caption generation method")
    parser.add_argument("--setting", type=str, default="", required=False, help="The setting name that you want to save the results as")
    parser.add_argument("--model_name", type=str, default=None, required="mistralai/Mistral-7B-Instruct-v0.1",\
        help="The pretrained model that your model is (finetuned from)")
    parser.add_argument("--model_checkpoint", type=str, default=None,required=False, help="Your model_checkpoint")
    parser.add_argument("--num_generation", type=int, default=10, required=False, help="Number of caption generations per contest")
    parser.add_argument("--new_padding_token", action="store_true", \
        help="Add a new padding token to the tokenizer, please keep it consistent with your model checkpoint")
    # Need to explicitly set the device for llava model than simply using CUDA_VISIBLE_DEVICES 
    parser.add_argument("--device", type=str, default='cuda:5', required=False, help="Set cuda device for llava model")
    
    args = parser.parse_args()
    main(args)
