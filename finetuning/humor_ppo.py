# This script is modified from the example script in the following repository:
# https://github.com/huggingface/trl/ 
# limitations under the License.
from dataclasses import dataclass, field
import os
from typing import Optional
import torch
from accelerate import Accelerator
from datasets import load_from_disk
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline, HfArgumentParser, AutoModelForCausalLM, AutoModelForSequenceClassification, GenerationConfig
from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed, PreTrainedModelWrapper
import wandb
from transformers.utils import logging
from util import seed_everything
seed_everything(2024)

tqdm.pandas()

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name= "mistralai/Mistral-7B-Instruct-v0.1",
            query_dataset="",
            reward_model="",
            learning_rate=1.41e-5,
            log_with="wandb",
            mini_batch_size=1,
            batch_size=128,
            gradient_accumulation_steps=128,
            early_stopping=False,
            target_kl= 50, 
            kl_penalty="kl",
            seed=2024,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    target_kl: float = 50
    logging_steps: int = 10  # match results in blog post
    save_steps: int = 10
    dataset_dir: str = "/your/dataset/path/"
    reward_model: str = "/your/reward/model/path"
    output_dir: str = "/your/output/directory/"
    """output directory"""
    run_name: str = "PPO"
    """run name"""
    use_seq2seq: bool = False
    """whether to use seq2seq models"""
    use_peft: bool = True
    """whether to use peft"""
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    trust_remote_code: bool = field(default=False, metadata={"help": "Enable `trust_remote_code`"})

if __name__ == "__main__":
    parser = HfArgumentParser(ScriptArguments)
    args = parser.parse_args_into_dataclasses()[0]
    args.ppo_config.output_dir = os.path.join(args.output_dir, 'ppo', args.ppo_config.model_name, args.run_name)
    args.ppo_config.target_kl = args.target_kl

    if not os.path.exists(args.ppo_config.output_dir):
        # Create the folder if it doesn't exist
        os.makedirs(args.ppo_config.output_dir)

    WANDB_LOG_MODEL = True
    wandb.init(project="ppo", name= args.ppo_config.model_name + "/" + args.ppo_config.output_dir)

    # We then define the arguments to pass to the sentiment analysis pipeline.
    # We set `return_all_scores` to True to get the sentiment score for each token.
    sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

    trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

    tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    # We retrieve the dataloader by calling the `build_dataset` function.
    train_dataset = load_from_disk(os.path.join(args.dataset_dir, 'dpo_dataset', 'train_dpo_dataset'))

    def tokenize_function(prompts):
        result = tokenizer([t for t in prompts['prompt'] ] ) 
        result["query"] = prompts["prompt"]
        return result
    dataset = train_dataset.map(tokenize_function, batched = True)
    print(dataset)

    def collator(data):
        return dict((key, [d[key] for d in data]) for key in data[0])

    # Now let's build the model, the reference model, and the tokenizer.
    if not args.use_peft:
        ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=args.trust_remote_code)
        device_map = None
        peft_config = None
    else:
        peft_config = args.peft_config
        ref_model = None
        # Copy the model to each device
        device_map = {"": Accelerator().local_process_index}

    model = trl_model_class.from_pretrained(
        args.ppo_config.model_name,
        trust_remote_code=args.trust_remote_code,
        device_map="cuda",
        peft_config=peft_config,
    )

    model.config.pad_token_id = model.config.eos_token_id

    generation_config = GenerationConfig.from_pretrained(args.ppo_config.model_name)
    generation_config.max_new_tokens = 256 # maximum number of new tokens that can be generated by the model
    generation_config.temperature = 0.7 # randomness of the generated tex
    generation_config.top_p = 0.95 # diversity of the generated text
    generation_config.do_sample = True # sampling during the generation process
    generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text

    generation_kwargs = generation_config.to_dict()

    # We then build the PPOTrainer, passing the model, the reference model, the tokenizer
    ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
    print("is peft model", ppo_trainer.is_peft_model)

    # We then build the sentiment analysis pipeline, passing the model name and the
    # sentiment analysis pipeline arguments. Let's also make sure to set the device
    # to the same device as the PPOTrainer.
    device = "auto"

    # We load the reward model into CPU, as fitting both the PPO and reward model on GPU can sometimes cause CUDA memory issues.
    reward_model = AutoModelForSequenceClassification.from_pretrained(
        args.reward_model,
        device_map="cpu",
        num_labels=1,
    )
    reward_model.config.pad_token_id = reward_model.config.eos_token_id
    sentiment_pipe = pipeline(
        "sentiment-analysis",
        model=reward_model,
        tokenizer=tokenizer,
    )

    logging.get_logger("transformers").setLevel(logging.ERROR)

    if not os.path.exists(args.ppo_config.output_dir):
        # Create the directory
        os.makedirs(args.ppo_config.output_dir)

    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
        print("epoch", epoch)
        query_tensors = batch["input_ids"]
        query_tensors = [torch.tensor(query_tensor) for query_tensor in query_tensors]
        
        response_tensors, ref_response_tensors = ppo_trainer.generate(
            query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs
        )
        
        batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
        batch["ref_response"] = tokenizer.batch_decode(ref_response_tensors, skip_special_tokens=True)

        # Compute sentiment score
        texts = [q + r for q, r in zip(batch["query"], batch["response"])]
        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
        rewards = [torch.tensor(output[0]["score"]) for output in pipe_outputs]
        ref_texts = [q + r for q, r in zip(batch["query"], batch["ref_response"])]
        ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)
        ref_rewards = [torch.tensor(output[0]["score"]) for output in ref_pipe_outputs]
        batch["ref_rewards"] = ref_rewards
        batch["rewards"] = rewards
        for metric in ["query", "response", "ref_response", "rewards", "ref_rewards"]:
            print(metric, batch[metric][:3])
        print('avg rewards', sum(rewards)/len(rewards), 'avg ref reward', sum(ref_rewards)/len(ref_rewards))
        # Run PPO step
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=["query", "response", "ref_response", "ref_rewards"])
        save_dir = os.path.join(args.ppo_config.output_dir,"checkpoint-"+str(epoch))

        if epoch in [1,2,5,7]:
            # More frequent saving for the first few iterations
            if not os.path.exists(save_dir):
                # Create the directory
                os.makedirs(save_dir)
            ppo_trainer._save_pretrained(save_dir)

        if epoch % args.save_steps == 0:
            if not os.path.exists(save_dir):
                # Create the directory
                os.makedirs(save_dir)
            ppo_trainer._save_pretrained(save_dir)